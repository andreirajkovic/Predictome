import itertools
import numpy as np
import re
from scipy import sparse
import pandas as pd
import pickle
from Matrices.src.featureFilter import filter_palette
from Classifiers.src.utilities import removeInvariant, removeMissing_classSize, generalFisherExact_sparse, general_chi_squared_sparse
from Matrices.src.getAttributes import parse_VCF
from sklearn.preprocessing import Imputer
from collections import defaultdict


def make_sparse_csr(array):
    """
    simple conversion to a sparse matrix

    Parameters:
    ======================================

    numpy array

    Notes:
    ======================================

    None

    Examples:
    ======================================

    >>>  new_sparse_matrix = pme.variant.make_sparse_csr(matrix)

    """
    return sparse.csr_matrix(array)


def save_sparse_csr(filename, array):
    """
    function to save the csr matrix this was taken from a stackoverflow post
    The matrix is saved to a designated folder.

    Parameters:
    ======================================

    filename -> string, name of file not path as this is saved to the matrices/output folder
    array -> numpy array

    Notes:
    ======================================

    None

    Examples:
    ======================================

    >>>  new_sparse_matrix = pme.variant.save_sparse_csr('file') ,matrix)

    """
    array = make_sparse_csr(array)
    np.savez('Matrices/output/' + filename,
             data=array.data, indices=array.indices,
             indptr=array.indptr, shape=array.shape)


def load_sparse_csr(filename):
    """
    function to load the csr matrix this was taken from a stackoverflow post

    Parameters:
    ======================================

    filename -> string, path to file

    Notes:
    ======================================

    None

    Examples:
    ======================================

    >>>  new_sparse_matrix = pme.variant.load('Matrices/output/file')

    """
    loader = np.load(filename)
    return sparse.csr_matrix((loader['data'], loader['indices'], loader['indptr']), shape=loader['shape'])


class matrix_shell(object):
    def __init__(self, class_folders=None, pVCF=None):
        """
        Generic class to hold the variables of the matrix and if necessary build the matrix
        from a VCF file.

        Parameters:
        ======================================

        class_folders -> list of strings that contain the file paths of the class folders assumed to be
        in Matrices/input/VCF

        pVCF -> pickled file that is of the parsed VCF content generated by the getAttributes class

        Notes:
        ======================================

        This class is used by the main class Predictome and generally should not be used as a
        standalone though there is nothing preventing one from doing so.


        Examples:
        ======================================

        None
        """
        if class_folders:
            self.filename = '_'.join(class_folders)
        self.file_paths = class_folders
        if pVCF is None:
            self.atr = parse_VCF(class_folders)
        else:
            self.atr = pVCF
        self.atr.row_count = 0
        self.atr.row_pos = {}
        self.start_index = []
        for cohort in self.file_paths:
            self.start_index.append(self.atr.row_count)
            self.atr.row_crawl(root=[cohort])
        self.original_cols = 0
        self.tcoords = 0
        self.fisher_coords = False

    def make_matrix(self):
        """
        initate the construction of the samples by snp matrix from scratch

        Parameters:
        ======================================

        None

        Notes:
        ======================================

        Use this function when no previous attempts have been made to generate a
        matrix. Otherwise use the custom matrix class to load previously generated
        matrices.

        Examples:
        ======================================

        >>> variant = pme.build_from_VCF(matrix_type='variant',
                                    vcf_folders_by_labels=['CTRL','CASE'])
        >>> pme.variant.make_matrix()

        """
        self.populate()
        self.columns2df()
        self.original_cols = np.arange(self.gt_matrix.shape[1])

    def reliability(self):
        """
        Count the number of missing values for each column and append to dataframe

        Parameters:
        ======================================

        None

        Notes:
        ======================================

        Use this function to compute the percent missing values that were imputed
        for each column. This function takes the class columns_df and added a new
        column to the dataframe called relability_score. This function should be
        executed before imputating the missing values.

        Examples:
        ======================================

        >>> pme.variant.reliability()

        """
        coords = self.gt_matrix == -1
        row, col, _ = sparse.find(coords)
        col_index, missing_count = np.unique(col, return_counts=1)
        print("These are the number of missing values %d" % missing_count.sum())
        relability_score = 1 - missing_count / self.gt_matrix.shape[0]
        # attach the scores onto the dataframe
        # A value closer to 1 indicates fewer missing
        self.columns_df['reliability'] = 1
        self.columns_df.loc[col_index, 'reliability'] = relability_score

    def impute(self, strategy='lazy', in_place=True):
        """
        Impute the missing values in the data matrix

        Parameters:
        ======================================

        strategy -> string: 'lazy', 'mode'

        in_place -> boolean: modify the gt_matrix or return a new copy of the imputed matrix

        Notes:
        ======================================

        The quickest strategy is to use the lazy method which will change all the missing
        values into 0s. Mode implements the scikit imputer class to select values that
        are the most frequent in that column. This should really only be used during training
        otherwise we might be snooping. A future version will remove the ability to impute
        by mode and replace it with the IMPUTER2 software.

        Examples:
        ======================================

        >>> pme.variant.impute(strategy='lazy', in_place=True)

        """
        if strategy is 'mode':
            X = self.gt_matrix[:, :-1].A
            y = self.gt_matrix[:, -1]
            imp = Imputer(missing_values=-1, strategy='most_frequent', axis=0)
            X_imp = imp.fit(X).transform(X)
            self.original_cols = self.original_cols[np.where(
                ~np.isnan(imp.statistics_))[0]]
            X_imp_sparse = sparse.csr_matrix(X_imp)
            if in_place is True:
                self.gt_matrix = sparse.hstack([X_imp_sparse, y])
            else:
                imputed_matrix = sparse.hstack([X_imp_sparse, y])
                return imputed_matrix
        elif in_place is True and strategy is 'lazy':
            self.gt_matrix[self.gt_matrix == -1] = 0
        elif in_place is False and strategy is 'lazy':
            fill_missing = self.gt_matrix
            fill_missing[fill_missing == -1] = 0
            return fill_missing

    def filter_matrix(self, by=None, threshold=1, tables=None, save=False):
        """
        Filter matrix based on predetermined properities of the SNPs

        Parameters:
        ======================================

        by -> string: 'keep_annotated', 'discard_annotated', 'invariant', 'missing', 'fisherp'

        tables -> pandas dataframe containing information about the SNPs/gene

        save -> boolean: save the filtered matrices or not

        Returns:
        ======================================

        Nothing

        Notes:
        ======================================

        Filtering your data is usually a domain specific matter. Therefore the tables
        that bear the information about the features in your table need to be generated
        first. Usually this is done with the help of Hadoop or ANNVOR. In this
        case we are using hadoop to filter feature information from the dbSNP3.4a
        database. Once we have our tables that have some quantitative or qualitative
        information about our SNPs, we then execute this function and choose a method
        of filtering. For example you are searching for SNPs that are important in dementia.
        Then you can look at genes that are only expressed in the brain. Or maybe you want
        to look at mutations that have in silico been computed to have a damaging effect.
        It is important to note that domain filtering does not always lead to better results.
        In fact, using regularization such as l1 or l2 may yield higher accuracy. However,
        if the data is full of suprious features, then filtering can help prevent the model
        from being mislead.

        Some basic filtering options include the removal of columns where the missing values
        equate to the size of a class. This can be done prior to training since these
        columns will never play an informative role in the machine learning process.
        Invariant columns can also be filtered out as they will usually never be
        informative (this is usually best carried out after imputation and should really
        be part of the training process). FisherExact will select columns that are only
        significant p < 0.05, but this type of filter must be executed only during cross validation
        otherwise significant overfitting will occur in combination with other regularizers. Also
        the matrix must always be in a sparse csr format for purposes of speed.

        Examples:
        ======================================
        >>> gene_annotated = pme.tables[0]
        >>> pme.variant.filter_matrix(by='keep_annotated', tables=gene_annotated)
        >>> pme.variant.filter_matrix(by='missing')
        >>> pme.variant.impute()
        >>> pme.variant.filter_matrix(by='invariant')

        """

        raw_matrix = self.gt_matrix[:, :-1]  # remove the label
        labels = self.gt_matrix[:, -1].A
        
        if 'keep_annotated' in by:
            palette = filter_palette(self.columns_df, save)
            palette.choose_data_from_df(tables)
            dom_columns, filtered_matrix = palette.build_keep_filtered_matrix(
                raw_matrix)
            self.original_cols = self.original_cols[dom_columns]
            self.gt_matrix = sparse.hstack([filtered_matrix, labels])
        
        elif 'discard_annotated' in by:
            dom_columns, filtered_matrix = palette.build_remove_filtered_matrix(
                raw_matrix)
            self.original_cols = self.original_cols[dom_columns]
            self.gt_matrix = sparse.hstack([filtered_matrix, labels]).tocsr()
        
        elif 'invariant' in by:
            v_columns, filtered_matrix = removeInvariant(raw_matrix)
            self.original_cols = self.original_cols[v_columns]
            self.gt_matrix = sparse.hstack([filtered_matrix, labels]).tocsr()
        
        elif 'missing' in by:
            miss_columns, filtered_matrix = removeMissing_classSize(
                raw_matrix, labels.T[0])
            self.original_cols = self.original_cols[miss_columns]
            self.gt_matrix = sparse.hstack([filtered_matrix, labels]).tocsr()
        
        elif 'chi2' in by:
            refs = np.unique(labels, return_counts=1)[1]
            c_table = np.zeros([len(np.unique(labels)), raw_matrix.shape[1]], dtype=int)
            for u in np.unique(labels):
                c_table[u] = raw_matrix[np.where(labels == u)[0].tolist()].sum(axis=0)
            self.pval_arry = general_chi_squared_sparse(refs, c_table)
        
        elif 'fisherp' in by:
            # count the number of case variants at a site.
            if self.fisher_coords is False:
                self.compute_coords_for_fisher()
            col_prbs_d = np.zeros((raw_matrix.shape[1], 2), dtype=int)
            col_prbs_n = np.zeros((raw_matrix.shape[1], 2), dtype=int)
            if self.tcoords != 0:
                for x in self.tcoords:
                    if x in self.D_dic:
                        Dindex, D_counts = np.unique(self.D_dic[x], return_counts=1)
                        col_prbs_d[Dindex, 1] += D_counts
                    if x in self.N_dic:
                        Nindex, N_counts = np.unique(self.N_dic[x], return_counts=1)
                        col_prbs_n[Nindex, 1] += N_counts
                # count the number of ctrl variants at a site
                labels = self.gt_matrix[self.tcoords, -1].A
                index, counts = np.unique(labels, return_counts=1)
                col_prbs_n[:, 0] = counts[0] - col_prbs_n[:, 1]
                col_prbs_d[:, 0] = counts[1] - col_prbs_d[:, 1]
            else:
                D_idx = np.where(labels == 1)[0]
                N_idx = np.where(labels == 0)[0]
                col_prbs_d[:, 1] = raw_matrix[D_idx, :].sum(axis=0).reshape(raw_matrix.shape[1])
                col_prbs_n[:, 1] = raw_matrix[N_idx, :].sum(axis=0).reshape(raw_matrix.shape[1])
                index, counts = np.unique(labels, return_counts=1)
                if isinstance(raw_matrix, loci_matrix) is True:
                    col_prbs_n[:, 0] = 2 * counts[0] - col_prbs_n[:, 1]
                    col_prbs_d[:, 0] = 2 * counts[1] - col_prbs_d[:, 1]
                else:
                    col_prbs_n[:, 0] = counts[0] - col_prbs_n[:, 1]
                    col_prbs_d[:, 0] = counts[1] - col_prbs_d[:, 1]

            pval_arry, odds_ratio = generalFisherExact_sparse(col_prbs_n, col_prbs_d, counts[0], counts[1])
            self.pval_arry = pval_arry
            self.odds_ratio = odds_ratio

    def compute_coords_for_fisher(self):
        """
        Speed up hack to simplify the downstream fisher computations

        Parameters:
        ======================================

        None

        Returns:
        ======================================

        when by is fisherp
        returns columns and matrix

        Notes:
        ======================================

        Builds a dictionary with the row index as the key and the column values as the value.
        The purpose being that the training coordinates generated from either
        cross validation or boostrapping can quickly be accessed from the dictionary
        to build the counts array that is then sent to the fisher-p exact function.

        Examples:
        ======================================

        None

        """
        labels = self.gt_matrix[:, -1].A
        D_idx = np.where(labels == 1)[0]
        N_idx = np.where(labels == 0)[0]
        D_coords = self.gt_matrix[D_idx, :-1] == 1
        N_coords = self.gt_matrix[N_idx, :-1] == 1
        self.Dr, self.Dc, _ = sparse.find(D_coords)
        self.Nr, self.Nc, _ = sparse.find(N_coords)
        # hash the rows
        self.D_dic = defaultdict(list)
        for i, v in enumerate(self.Dr):
            self.D_dic[v].append(self.Dc[i])
        self.N_dic = defaultdict(list)
        for i, v in enumerate(self.Nr):
            self.N_dic[v].append(self.Nc[i])
        self.fisher_coords = True
        print("comptued the fisher coordinates")

    def save(self, s):
        """
        Save the most relevant variables to rebuild the matrix

        Parameters:
        ======================================

        s -> string filename for the matrix be sure to add .npz

        Returns:
        ======================================

        None

        Notes:
        ======================================

        None

        Examples:
        ======================================

        pme.varaint.save('matrix.npz')

        """
        pickle.dump(self.atr, open('Matrices/output/pVCF.p', 'wb'))
        pickle.dump(self.columns_df, open('Matrices/output/df.p', 'wb'))
        save_sparse_csr('%s' % s, self.gt_matrix)
        print("Saved data")


class loci_matrix(matrix_shell):
    def __init__(self, class_folders=['testing_ctrl', 'testing_case'],
                 pVCF=None):
        """
        loci matrix also known as the genetic additive model

        Parameters:
        ======================================

        class_folders -> list of strings that denote the folder name containing
        the VCF files for a given class

        Returns:
        ======================================

        None

        Notes:
        ======================================

        The loci matrix encodes a strong linear representation of the features, meaning an individual can have 2x of a mutation, 1x of a mutation or 0x of a mutation all in a single feature. While the loci matrix is a denser representation, there is a loss of information as one cannot distinguish between the different kinds of mutations that happen at that same position e.g. chromosome 12 position 201231 has the mutations A and T at that site, but are numerically represented as being equal.

        Examples:
        ======================================
        pme = Predictome()
        pme.build_from_VCF(matrix_type='loci', vcf_folders_by_labels=['testing_ctrl','testing_case'])
        pme.loci.make_matrix()

        """
        matrix_shell.__init__(self, class_folders=class_folders, pVCF=pVCF)
        self.columns_df = 0
        self.gt_matrix = 0

    def __str__(self):
        return "The loci matrix"

    def build_dim(self, row_list, col_list, val_list):
        """
        Define the dimensions of the sparse coo_matrix

        Parameters:
        ======================================

        row_list -> list of rows (samples) that are parsed from the vcf

        col_list -> list of columns (features) that are parsed from the vcf

        val_list -> these are the hits (e.g. 1,2)

        Returns:
        ======================================

        None

        Notes:
        ======================================

        Used by the populate definition in order to create the
        sparse matrix representation directly

        Examples:
        ======================================

        None

        """
        print(len(self.atr.row_pos), np.max(row_list) + 1, self.atr.loci_count, np.max(col_list) + 1)
        cols = np.max(col_list) + 1
        rows = np.max(row_list) + 1
        labels = np.zeros(rows, dtype=np.int8).reshape((rows, 1))
        for i, _ in enumerate(self.start_index):
            if i + 1 < len(self.start_index):
                start = self.start_index[i]
                end = self.start_index[i + 1]
                labels[start:end] = i
            elif i + 1 == len(self.start_index):
                start = self.start_index[i]
                labels[start:] = i
        matrix = sparse.coo_matrix((val_list, (row_list, col_list)), shape=(rows, cols))
        self.gt_matrix = sparse.hstack((matrix, labels)).tocsr()

    def populate(self):
        """
        crawls throughout the folders for each label, populating 
        the main matrix by row and column. Column is retrieved from
        the column_pos dictioanry using the chromosome_position information
        parsed from the vcf file as input. The row position is determined 
        by the length of the samples
        """
        row_list = []
        col_list = []
        val_list = []
        for cohort in self.file_paths:
            paths = self.atr.import_files([cohort])
            for path in paths:
                samples = self.atr.sample_names(path)
                rows = [[self.atr.row_pos[x]] for x in samples]
                print("File %s contains # %s of samples" %
                      (path, len(samples)))
                with open(path, 'r') as fp:
                    for line in fp:
                        if not line.startswith("#"):
                            raw = line.split("\t")
                            chrm_pos_ref_alt = raw[0] + "_" + raw[1] + "_"  \
                                                      + raw[3] + "_" + raw[4]
                            genotype = re.findall('((?<=\\t).\/.)', line)
                            genotype = self.atr.genotypeparser(genotype)
                            if len(chrm_pos_ref_alt) == 0 or chrm_pos_ref_alt in self.atr.blacklisted_cols:
                                continue
                            elif chrm_pos_ref_alt in self.atr.columns_pos['loci']:
                                r, c, v = self.assign_row_column(
                                    rows, genotype, chrm_pos_ref_alt)
                                row_list += r
                                col_list += c
                                val_list += v
                fp.close()
        self.build_dim(row_list, col_list, val_list)

    def assign_row_column(self, rows, genotype, chrm_pos):
        """
        Convert the raw values from the VCF into a coo_matrix friendly format

        Parameters:
        ======================================

        rows ->  coordinates for where the samples should be in the design matrix

        genotype -> the converted genotype value collected from the VCF

        chrm_pos -> string that econdes the chromosome and position parsed from
                    the VCF file e.g. 12_123415

        Returns:
        ======================================

        row_list

        col_list

        val_list

        Notes:
        ======================================

        Used by the populate definition in order to create the
        sparse matrix representation directly

        Examples:
        ======================================

        None

        """
        row_list = []
        col_list = []
        val_list = []
        rows = np.asarray(rows)
        unique_genotypes = np.unique(genotype)
        gts = np.asarray(genotype)
        for gt in unique_genotypes:
            if gt != '0/0':
                cols_hit = self.atr.columns_pos['loci'][chrm_pos]  # inpt = ./. out = [123,124]
                rows_hit = rows[gts == gt].T[0].tolist()
                gt_hit = self.atr.genotypeparser([gt])[0][0]
                if isinstance(cols_hit, int):
                    r_c_v = list(itertools.chain(itertools.product(rows_hit, [cols_hit], [gt_hit])))
                else:
                    r_c_v = list(itertools.chain(itertools.product(rows_hit, cols_hit, [gt_hit])))
                row_list += [x[0] for x in r_c_v]
                col_list += [x[1] for x in r_c_v]
                val_list += [x[2] for x in r_c_v]
        return row_list, col_list, val_list

    def columns2df(self):
        """
        take the columns dictionary that has been generated and build a dataframe that can be used to keep track of the features when future filtration takes place. Format goes as such: index, chrm_pos, ref, alt, 2009349     21_17250144  C   T ref alt gets complicated when there are a multiallelic versions of the mutation for instance C,CTT how do we represent the alt position. In reality there are 3 possible representations [C] -> HET and HOMO, [C,CTT] -> HET,HET [CTT]-> HET,HOMO. Ideally we would have each column represent a single variant, but this changes the underlying structure of the model. Therefore in a sparse_loci representation of the data multiple allele information is going to be ignored by databases. But for dense_allele representations of the data we will be able to query specific columns based on the variant call.
        """
        index = []
        chrm_pos_col = []
        ref_col = []
        alt_col = []
        for key, val in self.atr.columns_pos['loci'].items():
            index.append(val)
            split_key = key.split("_")
            chrm_pos_col.append("_".join(split_key[:2]))
            ref_col.append(split_key[2])
            alt_col.append(split_key[3])
        column_df = pd.DataFrame(data={'chrm_pos': chrm_pos_col, 'ref': ref_col, 'alt': alt_col, 'chrm_pos_ref_alt': list(
            zip(chrm_pos_col, ref_col, alt_col))}, index=index)
        column_df['chrm_pos_ref_alt'] = column_df['chrm_pos'].map(
            str) + '_' + column_df['ref'].map(str) + '_' + column_df['alt'].map(str)
        self.columns_df = column_df


class allele_matrix(matrix_shell):
    def __init__(self, class_folders=['testing_ctrl', 'testing_case'],
                 pVCF=None):
        """
        allele matrix more flexible than the loci matrix

        Parameters:
        ======================================

        class_folders -> list of strings that denote the folder name containing
        the VCF files for a given class

        pVCF -> optional pickled file that contains the attributes of a previously parsed VCF

        Returns:
        ======================================

        None

        Notes:
        ======================================

        The allele matrix represents each individual mutation as a feature, even if
        the positions are the same. The allele matrix also maintains the linear
        hierarchy that is present in the loci matrix.

        Examples:
        ======================================
        pme = Predictome()
        pme.build_from_VCF(matrix_type='allele', vcf_folders_by_labels=['testing_ctrl','testing_case'])
        pme.allele.make_matrix()

        """

        matrix_shell.__init__(self, class_folders=class_folders, pVCF=pVCF)
        self.columns_df = 0
        self.gt_matrix = 0

    def build_dim(self, row_list, col_list, val_list):
        print(len(self.atr.row_pos), np.max(row_list) + 1, self.atr.allele_count, np.max(col_list) + 1)
        cols = np.max(col_list) + 1
        rows = np.max(row_list) + 1
        labels = np.zeros(rows, dtype=np.int8).reshape((rows, 1))
        for i, _ in enumerate(self.start_index):
            if i + 1 < len(self.start_index):
                start = self.start_index[i]
                end = self.start_index[i + 1]
                labels[start:end] = i
            elif i + 1 == len(self.start_index):
                start = self.start_index[i]
                labels[start:] = i
        matrix = sparse.coo_matrix((val_list, (row_list, col_list)), shape=(rows, cols))
        self.gt_matrix = sparse.hstack((matrix, labels)).tocsr()

    def populate(self):
        """
        Very similar to loci_populate, only difference is that columns are retrieved
        from the column_pos['allele] key using the chromosome_position and allele info,
        due to the columns being slightly less linear in nature.
        """
        row_list = []
        col_list = []
        val_list = []
        for cohort in self.file_paths:
            paths = self.atr.import_files([cohort])
            for path in paths:
                samples = self.atr.sample_names(path)
                rows = [[self.atr.row_pos[y]] for y in samples]
                print("File %s contains # %s of samples" %
                      (path, len(samples)))
                with open(path, 'r') as fp:
                    for line in fp:
                        if not line.startswith("#"):
                            raw = line.split("\t")
                            chrm_pos_ref_alt = raw[0] + "_" + raw[1] + "_"  \
                                                      + raw[3] + "_" + raw[4]
                            genotype = re.findall('((?<=\\t).\/.)', line)
                            if len(chrm_pos_ref_alt) == 0 or chrm_pos_ref_alt in self.atr.blacklisted_cols:
                                continue
                            elif chrm_pos_ref_alt in self.atr.columns_pos['allele']:
                                r, c, v = self.assign_row_column(
                                    rows, genotype, chrm_pos_ref_alt)
                                row_list += r
                                col_list += c
                                val_list += v
                fp.close()
        self.build_dim(row_list, col_list, val_list)

    def assign_row_column(self, rows, genotype, chrm_pos):
        row_list = []
        col_list = []
        val_list = []
        unique_genotypes = np.unique(genotype)
        rows = np.asarray(rows)
        gts = np.asarray(genotype)
        for gt in unique_genotypes:
            if gt != '0/0':
                allele = self.genotype2allele(gt)
                cols_hit = self.atr.columns_pos['allele'][chrm_pos][allele]
                rows_hit = rows[gts == gt].T[0].tolist()
                gt_hit = self.atr.genotypeparser([gt])
                if isinstance(cols_hit, int):
                    r_c_v = list(itertools.chain(itertools.product(rows_hit, [cols_hit], [gt_hit])))
                else:
                    r_c_v = list(itertools.chain(itertools.product(rows_hit, cols_hit, [gt_hit])))
                row_list += [x[0] for x in r_c_v]
                col_list += [x[1] for x in r_c_v]
                val_list += [x[2] for x in r_c_v]
        return row_list, col_list, val_list

    def genotype2allele(self, gt):
        allele = [max(tuple(int(y) for y in gt.split("/")))][0]
        return allele

    def columns2df(self):
        index = []
        chrm_pos_col = []
        ref_col = []
        alt_col = []
        allele_val = []
        for key, val in self.atr.columns_pos['allele'].items():
            for k, v in val.items():
                index.append(v)
                split_key = key.split("_")
                chrm_pos_col.append("_".join(split_key[:2]))
                ref_col.append(split_key[2])
                allele_val.append(k)
                alt_col.append(split_key[3])
        column_df = pd.DataFrame(data={'chrm_pos': chrm_pos_col, 'ref': ref_col, 'alt': alt_col,
                                       'allele': allele_val, 'chrm_pos_ref_alt': list(zip(chrm_pos_col, ref_col, alt_col))}, index=index)
        column_df['chrm_pos_ref_alt'] = column_df['chrm_pos'].map(
            str) + '_' + column_df['ref'].map(str) + '_' + column_df['alt'].map(str)
        self.columns_df = column_df


class variant_matrix(matrix_shell):
    def __init__(self, class_folders=['ESP6900', 'FPOF', 'IPOF', 'PPOF'],
                 pVCF=None):
        """
        variant matrix more flexible than the allele matrix

        Parameters:
        ======================================

        class_folders -> list of strings that denote the folder name containing
        the VCF files for a given class

        pVCF -> optional pickled file that contains the attributes of a previously parsed VCF

        Returns:
        ======================================

        None

        Notes:
        ======================================

        The most flexible format is the variant format, which only differs from the allele matrix
        in the sense that the linear mapping of the mutations does not exist. Therefore,
        being homozygous for a mutation would be strictly categorical. A direct consequence of
        using the variant matrix is the potential for more non-linear interactions as one can
        be homozygous for a mutation and not be associated with the disease, while being heterozygous
        is associated with the disease.

        Examples:
        ======================================
        pme = Predictome()
        pme.build_from_VCF(matrix_type='variant', vcf_folders_by_labels=['testing_ctrl','testing_case'])
        pme.variant.make_matrix()

        """

        matrix_shell.__init__(self, class_folders=class_folders, pVCF=pVCF)
        self.columns_df = 0
        self.gt_matrix = 0

    def build_dim(self, row_list, col_list, val_list):
        print(len(self.atr.row_pos), np.max(row_list) + 1, self.atr.variant_count, np.max(col_list) + 1)
        cols = np.max(col_list) + 1
        rows = np.max(row_list) + 1
        labels = np.zeros(rows, dtype=np.int8).reshape((rows, 1))
        for i, _ in enumerate(self.start_index):
            if i + 1 < len(self.start_index):
                start = self.start_index[i]
                end = self.start_index[i + 1]
                labels[start:end] = i
            elif i + 1 == len(self.start_index):
                start = self.start_index[i]
                labels[start:] = i
        matrix = sparse.coo_matrix((val_list, (row_list, col_list)), shape=(rows, cols))
        self.gt_matrix = sparse.hstack((matrix, labels)).tocsr()

    def populate(self):
        """
        Very similar to ppoulate, only difference is that columns are retrieved
        from the column_pos dictioanry using the chromosome_position and genotype info,
        due to the columns being cateogorical in nature.
        """
        row_list = []
        col_list = []
        val_list = []
        for cohort in self.file_paths:
            paths = self.atr.import_files([cohort])
            for path in paths:
                samples = self.atr.sample_names(path)
                rows = [[self.atr.row_pos[y]] for y in samples]
                print("File %s contains # %s of samples" %
                      (path, len(samples)))
                with open(path, 'r') as fp:
                    for line in fp:
                        if not line.startswith("#"):
                            raw = line.split("\t")
                            chrm_pos_ref_alt = raw[0] + "_" + raw[1] + "_"  \
                                                      + raw[3] + "_" + raw[4]
                            genotype = re.findall('((?<=\\t).\/.)', line)
                            if len(chrm_pos_ref_alt) == 0 or chrm_pos_ref_alt in self.atr.blacklisted_cols:
                                continue
                            elif chrm_pos_ref_alt in self.atr.columns_pos['variant']:
                                r, c, v = self.assign_row_column(
                                    rows, genotype, chrm_pos_ref_alt)
                                row_list += r
                                col_list += c
                                val_list += v
                fp.close()
        self.build_dim(row_list, col_list, val_list)

    def assign_row_column(self, rows, genotype, chrm_pos):
        row_list = []
        col_list = []
        val_list = []
        rows = np.asarray(rows)
        unique_genotypes = np.unique(genotype)
        gts = np.asarray(genotype)
        for gt in unique_genotypes:
            if gt != '0/0':
                cols_hit = self.atr.columns_pos['variant'][chrm_pos][gt]
                rows_hit = rows[gts == gt].T[0].tolist()
                gt_hit = self.atr.genotypeparser([gt])[0][0]
                if gt_hit is 2:
                    gt_hit = 1
                if isinstance(cols_hit, int):
                    r_c_v = list(itertools.chain(itertools.product(rows_hit, [cols_hit], [gt_hit])))
                else:
                    r_c_v = list(itertools.chain(itertools.product(rows_hit, cols_hit, [gt_hit])))
                row_list += [x[0] for x in r_c_v]
                col_list += [x[1] for x in r_c_v]
                val_list += [x[2] for x in r_c_v]
        return row_list, col_list, val_list

    def columns2df(self):
        index = []
        chrm_pos_col = []
        ref_col = []
        alt_col = []
        vcf_val = []
        for key, val in self.atr.columns_pos['variant'].items():
            for k, v in val.items():
                if k != './.':
                    index.append(v)
                    split_key = key.split("_")
                    chrm_pos_col.append("_".join(split_key[:2]))
                    ref_col.append(split_key[2])
                    vcf_val.append(k)
                    alt_col.append(split_key[3])
        column_df = pd.DataFrame(data={'chrm_pos': chrm_pos_col, 'ref': ref_col, 'alt': alt_col,
                                       'variant_call': vcf_val, 'chrm_pos_ref_alt': list(zip(chrm_pos_col, ref_col, alt_col))}, index=index)
        column_df['chrm_pos_ref_alt'] = column_df['chrm_pos'].map(
            str) + '_' + column_df['ref'].map(str) + '_' + column_df['alt'].map(str)
        self.columns_df = column_df


class custom_matrix(matrix_shell):
    def __init__(self, matrix, df_for_filter, row_data):
        """
        initate the construction of the samples by snp matrix from scratch

        Parameters:
        ======================================

        None

        Notes:
        ======================================

        Use this function when no previous attempts have been made to generate a
        matrix. Otherwise use the custom matrix class to load previously generated
        matrices.

        Examples:
        ======================================
        >>> matrix  = pme.load_sparse_csr('Matrices/output/UnfilteredMatrix/CTRL_CASE.npz')
        >>> df = pickle.load(open('Matrices/output/df.p','rb'))
        >>> pVCF = pickle.load(open('Matrices/output/pVCF.p','rb'))
        >>> row_data = pVCF.row_pos
        >>> pme.build_from_VCF(matrix_type='custom', matrix=matrix,
                              df_for_filter=df, row_data=row_data)

        """

        self.columns_df = df_for_filter
        self.gt_matrix = matrix
        self.row_pos = row_data  # in some cases we might pass pVCF here
        self.original_cols = np.arange(self.gt_matrix.shape[1])
        self.tcoords = 0
        self.fisher_coords = False
